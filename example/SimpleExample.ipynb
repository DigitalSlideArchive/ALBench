{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f297f83e-bf29-480c-9573-0631e2ba66cb",
   "metadata": {},
   "source": [
    "<h1>ALBench project: al_bench</h1>\n",
    "\n",
    "<p>This Jupyter lab demonstrates use of the al_bench Active Learning Benchmark Tool</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41732a2-e631-4af0-a79d-e943fb2dfa7c",
   "metadata": {},
   "source": [
    "<h2>Install needed Python packages</h2>\n",
    "\n",
    "<p>If you haven't yet installed these packages, remove the \"<code>#</code>\" characters and run this code block.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea9a01a-cf90-4477-802f-7cc37fa0a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ../../ALBench  # Installs al_bench and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439f464-a0b3-4fb3-8ca6-c83415f25403",
   "metadata": {},
   "source": [
    "<h2>Overview</h2>\n",
    "\n",
    "<p>The tool takes an input dataset, machine learning model, and active learning strategy and outputs information to be used in evaluating how well the strategy does with that model and dataset. By running the tool multiple times with different inputs, the tool allows comparisons across different active learning strategies and also allows comparisons across different models and across different datasets. Researchers can use the tool to test proposed active learning strategies in the context of a specific model and dataset; or multiple models and datasets can be used to get a broader picture of each strategy's effectiveness in multiple contexts. As an alternative use case, multiple runs of the tool with different models and datasets can be compared, evaluating these models and datasets for their compatibility with a given active learning strategy.</p>\n",
    "\n",
    "<p>In the present example, we will compare several active learning strategies, each employed on the same model and dataset.  To do this we will fetch a dataset and provide it to a dataset handler, and we will build a model and provide it to a model handler.  These are then used with each of the active learning strategy handlers.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd7e1bd-8315-413d-b3ee-2d38e0777819",
   "metadata": {},
   "source": [
    "<h2>Find a dataset and create a Dataset Handler</h2>\n",
    "\n",
    "<p>We fetch a dataset of 4598 feature vectors of length 1280 and the associated label for each feature vector.  The benchmarking tool requires that all examples be labeled, although the labels are not used initially.  The label for a sample is revealed to the machine learning training only when the active learning strategy indicates that the clinician has been asked to label that sample.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114537f1-77da-4bbc-841e-8f2ee9557282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 10:33:24.590112: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-13 10:33:24.741693: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-13 10:33:24.779296: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-13 10:33:25.713309: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/home/lee.newberg/Support/dakota-6.13.0.src-install/lib\n",
      "2023-02-13 10:33:25.713476: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/home/lee.newberg/Support/dakota-6.13.0.src-install/lib\n",
      "2023-02-13 10:33:25.713491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 4598 feature vectors of length 1280.\n",
      "Read in 4598 labels for the feature vectors.\n"
     ]
    }
   ],
   "source": [
    "import al_bench as alb\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "filename = \"../test/TCGA-A2-A0D0-DX1_xmin68482_ymin39071_MPP-0.2500.h5py\"\n",
    "with h5.File(filename) as ds:\n",
    "    my_feature_vectors = np.array(ds[\"features\"])\n",
    "    print(\n",
    "        f\"Read in {my_feature_vectors.shape[0]} feature vectors of length {my_feature_vectors.shape[1]}.\"\n",
    "    )\n",
    "    my_labels = np.array(ds[\"labels\"])\n",
    "    print(f\"Read in {my_labels.shape[0]} labels for the feature vectors.\")\n",
    "my_label_definitions = [\n",
    "    {\n",
    "        0: {\"description\": \"other\"},\n",
    "        1: {\"description\": \"tumor\"},\n",
    "        2: {\"description\": \"stroma\"},\n",
    "        3: {\"description\": \"infiltrate\"},\n",
    "    }\n",
    "]\n",
    "my_dataset_handler = alb.dataset.GenericDatasetHandler()\n",
    "my_dataset_handler.set_all_feature_vectors(my_feature_vectors)\n",
    "my_dataset_handler.set_all_label_definitions(my_label_definitions)\n",
    "my_dataset_handler.set_all_labels(my_labels)\n",
    "\n",
    "# Set aside disjoint sets of examples for use in validation and as the initial training set\n",
    "number_of_validation_indices = my_feature_vectors.shape[0] // 10\n",
    "number_of_initial_training = 20\n",
    "random_samples = random.sample(\n",
    "    range(my_feature_vectors.shape[0]),\n",
    "    number_of_validation_indices + number_of_initial_training,\n",
    ")\n",
    "my_dataset_handler.set_validation_indices(\n",
    "    np.array(random_samples[:number_of_validation_indices])\n",
    ")\n",
    "currently_labeled_examples = np.array(random_samples[number_of_validation_indices:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665ae47-e23f-461c-9167-4e4443c4fb81",
   "metadata": {},
   "source": [
    "<h2>Create a model and a Model Handler</h2>\n",
    "\n",
    "<p>Build a model that we will train.  We will build both a TensorFlow model and a PyTorch model.  As part of our comparison we could compare them, however we will not do so.  We'll choose one of them for use with the active learning strategies.  First we set some variables with common parameters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3576b1b9-a56a-44c0-ab72-3a4011d927ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_categories = len(my_label_definitions[0])\n",
    "number_of_features = my_feature_vectors.shape[1]\n",
    "hidden_units = 128\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e4959-ffd6-4041-bb0a-98f82cdfd770",
   "metadata": {},
   "source": [
    "<h3>Build a TensorFlow model and its Model Handler</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e914905-f264-4a40-84df-0e692c0c1ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow model handler built\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 10:33:28.148010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2006] Ignoring visible gpu device (device: 1, name: Quadro P400, pci bus id: 0000:a6:00.0, compute capability: 6.1) with core count: 2. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2023-02-13 10:33:28.148375: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-13 10:33:28.800814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22331 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:73:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "my_tensorflow_model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(number_of_features,)),\n",
    "        tf.keras.layers.Dense(hidden_units, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(dropout, noise_shape=None, seed=20220909),\n",
    "        tf.keras.layers.Dense(number_of_categories, activation=\"softmax\"),\n",
    "    ],\n",
    "    name=(\n",
    "        f\"{number_of_categories}_labels_from_{number_of_features}_features_with_\"\n",
    "        f\"dropout_{dropout}\"\n",
    "    ),\n",
    ")\n",
    "my_tensorflow_model_handler = alb.model.NonBayesianTensorFlowModelHandler()\n",
    "my_tensorflow_model_handler.set_model(my_tensorflow_model)\n",
    "print(\"Tensorflow model handler built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a077bc0-d613-470a-b33b-a10ce7592727",
   "metadata": {},
   "source": [
    "<h3>Build a Torch model and its Model Handler</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22abc29a-152a-4184-813a-5a66b8ed105c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model handler built\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyTorchModel(torch.nn.modules.module.Module):\n",
    "    def __init__(self, number_of_features, number_of_categories):\n",
    "        super(MyTorchModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(number_of_features, hidden_units)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.dropout1 = torch.nn.Dropout(p=dropout)\n",
    "        self.fc2 = torch.nn.Linear(hidden_units, number_of_categories)\n",
    "        self.softmax1 = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "my_torch_model = MyTorchModel(number_of_features, number_of_categories)\n",
    "\n",
    "my_pytorch_model_handler = alb.model.NonBayesianPyTorchModelHandler()\n",
    "my_pytorch_model_handler.set_model(my_torch_model)\n",
    "print(\"PyTorch model handler built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbd678-e488-4b99-912c-7318d5507be1",
   "metadata": {},
   "source": [
    "<h3>Choose one of the models to proceed with</h3>\n",
    "\n",
    "<p>The rest of the code is agnostic to whether one is using a TensorFlow or PyTorch model, or some of each.  One proceeds with whichever model handlers one wants to use.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9304dc37-2936-4b20-9096-2162390943bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model_handler = my_tensorflow_model_handler\n",
    "my_model_handler = my_pytorch_model_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e961066-33aa-4ce6-b13d-91927aa6102e",
   "metadata": {},
   "source": [
    "<h2>Make use of Strategy Handlers for active learning</h2>\n",
    "\n",
    "<p>Let's run and compare four active learning strategies.  Each strategy looks at the unlabeled samples, ranks them, and the selects the samples that appear to be the least certain predictions, by one of several evaluaiton methods.  Key to understanding these evaluation methods is understanding that a prediction for a sample is made by a machine learning algorithm by computing a score for each possible label -- the scores are nonnegative and sum to 1.0 -- and then chosing the label that scores highest.</p>\n",
    "\n",
    "<p>There are different ways to choose which unlabeled samples should be labeled next.  We will demonstrate four:\n",
    "<ol>\n",
    "    <li>\"Random\": Select the next samples randomly</li>\n",
    "    <li>\"LeastConfidence\": A sample's certainy is defined as the predicted label's score.</li>\n",
    "    <li>\"LeastMargin\": A sample's certainty is defined by the difference between the predicted label's score and the score of the second-best label.</li>\n",
    "    <li>\"Entropy\": A sample's scores are interpreted as a probability distribution and its entropy is computed.  Because high entropy means more uncertainty, a sample's certainty is defined to be the negative of the entropy.</li>\n",
    "</ol></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7331ecb0-90b8-4b51-b5f1-4c1e1bd855c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Begin Strategy 'Random' at 2023-02-13 10:33:29.341913 ===\n",
      "Training with 20 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 30 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 40 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 50 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 60 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 70 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 80 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 90 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 100 examples\n",
      "Predicting for 4598 examples\n",
      "=== Begin Strategy 'LeastConfidence' at 2023-02-13 10:33:58.095140 ===\n",
      "Training with 20 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 30 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 40 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 50 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 60 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 70 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 80 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 90 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 100 examples\n",
      "Predicting for 4598 examples\n",
      "=== Begin Strategy 'LeastMargin' at 2023-02-13 10:34:26.565446 ===\n",
      "Training with 20 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 30 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 40 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 50 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 60 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 70 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 80 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 90 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 100 examples\n",
      "Predicting for 4598 examples\n",
      "=== Begin Strategy 'Entropy' at 2023-02-13 10:34:54.286592 ===\n",
      "Training with 20 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 30 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 40 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 50 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 60 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 70 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 80 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 90 examples\n",
      "Predicting for 4598 examples\n",
      "Training with 100 examples\n",
      "Predicting for 4598 examples\n",
      "=== Done at 2023-02-13 10:35:22.370426 ===\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "all_logs_dir = \"runs-SimpleExample\"\n",
    "try:\n",
    "    shutil.rmtree(all_logs_dir)  # DELETE OLD LOG FILES\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for name, my_strategy_handler in (\n",
    "    (\"Random\", alb.strategy.RandomStrategyHandler()),\n",
    "    (\"LeastConfidence\", alb.strategy.LeastConfidenceStrategyHandler()),\n",
    "    (\"LeastMargin\", alb.strategy.LeastMarginStrategyHandler()),\n",
    "    (\"Entropy\", alb.strategy.EntropyStrategyHandler()),\n",
    "):\n",
    "    print(f\"=== Begin Strategy {repr(name)} at {datetime.now()} ===\")\n",
    "    my_strategy_handler.set_dataset_handler(my_dataset_handler)\n",
    "    my_strategy_handler.set_model_handler(my_model_handler)\n",
    "    my_strategy_handler.set_learning_parameters(\n",
    "        label_of_interest=0,  # We've supplied only one label per feature vector\n",
    "        maximum_queries=8,\n",
    "        number_to_select_per_query=10,\n",
    "    )\n",
    "\n",
    "    # ################################################################\n",
    "    # Simulate the strategy.\n",
    "    my_strategy_handler.run(currently_labeled_examples)\n",
    "    # ################################################################\n",
    "\n",
    "    # We will write out collected information to disk.  First say where:\n",
    "    log_dir = os.path.join(all_logs_dir, name)\n",
    "    # Write accuracy and loss information during training\n",
    "    my_strategy_handler.write_train_log_for_tensorboard(log_dir=log_dir)\n",
    "    # Write confidence statistics during active learning\n",
    "    my_strategy_handler.write_confidence_log_for_tensorboard(log_dir=log_dir)\n",
    "print(f\"=== Done at {datetime.now()} ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7135fa-8548-4162-a55c-32716ffe81f7",
   "metadata": {},
   "source": [
    "<h2>Use with TensorBoard</h2>\n",
    "\n",
    "<p>TensorBoard provides a way to graph the information from the log files we have written.  If it is not blocked by a firewall, the TensorBoard graphics will appear in this Jupyter lab.  Otherwise, the TensorBoard output can be made to appear in any web browser by launching \"<code>tensorboard --logdir runs</code>\" from a command prompt and then asking the web browser to load \"<code>http://localhost:6006/</code>\".</p>\n",
    "\n",
    "<p>Because these are randomized simulations you will not see the same output each time you run them.  Clicking on the \"Scalars\" tab allows one to change the smoothing of the displayed graphics, e.g., to 0.</p>\n",
    "\n",
    "<p>The Confidence graphs show how the certainty, among samples that are (simulated as) not yet labeled, changes during the active learning process; specifically, as a function of the number of samples that have been labeled so far.  For example, Confidence/margin/10% measures certainty for a sample's prediction as the difference between the two highest-scoring lablels, and the 10% indicates that this is the 10 percentile among all unlabeled samples -- which is among the worst performing of these samples.  Confidence/entropy/50% shows the median value among unlabeled samples of the negative entropy.  Confidence/maximum/5% shows the 5 percentile value -- among the very worst -- for a sample's maximum label score, which is the score for its predicted label.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b2589a-a665-4124-809a-d04cd114b71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cc8a6a687fff1089\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cc8a6a687fff1089\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {all_logs_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "832c1a17-b976-4773-9588-e99fe67ed891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "al_bench",
   "language": "python",
   "name": "al_bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
